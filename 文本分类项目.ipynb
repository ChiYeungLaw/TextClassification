{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_9J8dVMKbMb"
   },
   "source": [
    "# 项目描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iq38orwkKg5e"
   },
   "source": [
    "利用IMDB电影评论数据做文本分类项目，分别利用FNN、TextCNN、TextRNN、TextRCNN、GRU+Attention。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nW0GFyuPtUVO"
   },
   "source": [
    "# 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUIAoO9cC6_b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")\n",
    "os.chdir(\"/content/drive/My Drive\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nynlluqzE_2N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import vocab\n",
    "\n",
    "text_field = data.Field(tokenize='spacy', lower=True, include_lengths=True,\n",
    "                        fix_length=100)\n",
    "label_field = data.Field(sequential=False, use_vocab=False, dtype=torch.long)\n",
    "train, valid, test = data.TabularDataset.splits(path='',\n",
    "                                                train='imdb-train.csv',\n",
    "                                                validation='imdb-valid.csv',\n",
    "                                                test='imdb-test.csv',\n",
    "                                                format='csv', skip_header=True,\n",
    "                                                fields=[('sentence', text_field), ('label', label_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6STkFSQafSM"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoI5yPFIVEUG"
   },
   "outputs": [],
   "source": [
    "vec = vocab.Vectors(name='glove.6B.300d.txt')\n",
    "text_field.build_vocab(train, valid, test, max_size=250000, vectors=vec,\n",
    "                       unk_init=torch.Tensor.normal_)\n",
    "label_field.build_vocab(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzwZVu7EbZRY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), batch_sizes=(64, 64, 64),\n",
    "                                                               sort_key=lambda x: len(x.sentence),\n",
    "                                                               sort_within_batch=True,\n",
    "                                                               repeat=False, shuffle=True,\n",
    "                                                               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfDvZOj9w9_A"
   },
   "source": [
    "# 模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4eRtD_gxA3l"
   },
   "outputs": [],
   "source": [
    "def train_fun(model, train_iter, dev_iter, num_epoch, opt, criterion, eva,\n",
    "              out_model_file):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    best_dev_acc = 0.\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss = 0.\n",
    "        for batch in train_iter:\n",
    "            output = model(batch.sentence)\n",
    "            loss = criterion(output, batch.label)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        loss_list.append(total_loss)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            dev_acc = eva(model, dev_iter)\n",
    "            print(f\"Epoch: {epoch+1}/{num_epoch}. Total loss: {total_loss:.3f}. Validation Set Acc: {dev_acc:.3%}.\")\n",
    "            if dev_acc > best_dev_acc:\n",
    "                best_dev_acc = dev_acc\n",
    "                torch.save(model.state_dict(), out_model_file)\n",
    "            else:\n",
    "                if epoch + 1 > 60:\n",
    "                    print(\"Early Stop!\")\n",
    "                    break\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dY4gBmd3Xuv"
   },
   "source": [
    "# 模型评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lSdhhl03Zav"
   },
   "outputs": [],
   "source": [
    "def eva(model, data_iter):\n",
    "    correct, count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            pred = model(batch.sentence)\n",
    "            pred = torch.argmax(pred, dim=-1)\n",
    "            correct += (pred == batch.label).sum().item()\n",
    "            count += len(pred)\n",
    "    return correct / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6dPFDmWStZ_X"
   },
   "source": [
    "# 分类器1: TextFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAWd1-dNtltQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, seq_len, hidden_size1, hidden_size2,\n",
    "                 out_dim, pretrained_embed):\n",
    "        super(FNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embed, freeze=True)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * seq_len, hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size2, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = x # [seq, bs]\n",
    "        x = self.embed(x) # [seq, bs, embed]\n",
    "        x = x.permute(1, 0, 2) # [bs, seq, embed]\n",
    "        x = x.reshape(x.shape[0], -1) # [bs, seq * embed]\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "XOKzkQzYvjPJ",
    "outputId": "594397b8-1b3b-44c9-f50c-90838657bdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begin!\n",
      "Epoch: 1/10. Total loss: 318.329. Validation Set Acc: 48.720%.\n",
      "Epoch: 2/10. Total loss: 285.645. Validation Set Acc: 48.920%.\n",
      "Epoch: 3/10. Total loss: 283.161. Validation Set Acc: 62.320%.\n",
      "Epoch: 4/10. Total loss: 278.264. Validation Set Acc: 66.700%.\n",
      "Epoch: 5/10. Total loss: 217.278. Validation Set Acc: 69.860%.\n",
      "Epoch: 6/10. Total loss: 187.332. Validation Set Acc: 70.340%.\n",
      "Epoch: 7/10. Total loss: 163.138. Validation Set Acc: 70.180%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 300\n",
    "out_dim = 2\n",
    "pretrained_embed = text_field.vocab.vectors\n",
    "num_epoch = 10\n",
    "lr = 0.01\n",
    "seq_len = 100\n",
    "hidden_size1 = 250\n",
    "hidden_size2 = 200\n",
    "out_model_file = 'textfnn.pt'\n",
    "\n",
    "textfnn = FNN(embedding_dim, seq_len, hidden_size1, hidden_size2,\n",
    "              out_dim, pretrained_embed).to(device)\n",
    "opt = torch.optim.Adam(textfnn.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Training begin!\")\n",
    "loss_list = train_fun(textfnn, train_iter, valid_iter, num_epoch, opt, criterion, eva,\n",
    "                      out_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ka_PCUq7AkDf",
    "outputId": "f428fab1-6689-4879-c9db-b056aad60683"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70395"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva(textfnn, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lV8xCDOVE-yR"
   },
   "source": [
    "# 分类器2:TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAm4aeKqS6CZ"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_list):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=fs)\n",
    "            for fs in filter_list\n",
    "        ])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_params()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return [self.relu(conv(x)) for conv in self.convs]\n",
    "    \n",
    "    def init_params(self):\n",
    "        for m in self.convs:\n",
    "            nn.init.kaiming_normal_(m.weight.data)\n",
    "            nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpf1UZNmFCXV"
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embed_dim, pretrained_embed, filter_size, filter_channels,\n",
    "                 out_dim, dropout_rate):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embed, freeze=True)\n",
    "        self.cnn = CNN(embed_dim, filter_channels, filter_size)\n",
    "        self.proj = nn.Linear(len(filter_size) * filter_channels, out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        nn.init.xavier_normal_(self.proj.weight.data)\n",
    "        nn.init.constant_(self.proj.bias.data, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        text, _ = x\n",
    "        text = text.permute(1, 0) # [bs, seq_len]\n",
    "        out = self.embed(text) # [bs, seq, emb]\n",
    "        out = out.permute(0, 2, 1) # [bs, emb, seq]\n",
    "        out = self.cnn(out) # [nf, bs, fc, -1]\n",
    "        out = [torch.max_pool1d(x, x.shape[-1]).squeeze(2) for x in out] # [nf, bs, fc]\n",
    "        out = self.dropout(torch.cat(out, dim=1)) \n",
    "        return self.proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "dr7Mj8hTWTye",
    "outputId": "41c05ca8-b53a-4ed7-a9a8-eaeb970fb2f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100. Total loss: 204.973. Validation Set Acc: 75.440%.\n",
      "Epoch: 40/100. Total loss: 217.076. Validation Set Acc: 75.240%.\n",
      "Epoch: 60/100. Total loss: 194.838. Validation Set Acc: 75.420%.\n",
      "Epoch: 80/100. Total loss: 165.143. Validation Set Acc: 76.000%.\n",
      "Epoch: 100/100. Total loss: 170.690. Validation Set Acc: 76.220%.\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "pretrained_embed = text_field.vocab.vectors\n",
    "filter_size = [1, 2, 3, 4, 5]\n",
    "filter_channels = 200\n",
    "out_dim = 2\n",
    "dropout_rate = 0.2\n",
    "num_epoch = 100\n",
    "lr = 0.01\n",
    "out_model_file = 'textcnn.pt'\n",
    "\n",
    "textcnn = TextCNN(embed_dim, pretrained_embed, filter_size, filter_channels,\n",
    "                  out_dim, dropout_rate).to(device)\n",
    "opt = torch.optim.Adam(textcnn.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list = train_fun(textcnn, train_iter, valid_iter, num_epoch, opt, criterion, eva,\n",
    "                      out_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T1Wfr2cbYM79",
    "outputId": "ac862389-f3b2-4b01-f20a-9bc3235d277e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7583"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcnn = TextCNN(embed_dim, pretrained_embed, filter_size, filter_channels,\n",
    "                  out_dim, dropout_rate).to(device)\n",
    "textcnn.load_state_dict(torch.load('textcnn.pt'))\n",
    "eva(textcnn, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_yH31g7enS1"
   },
   "source": [
    "# 分类器3:TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okrtrPcHemUR"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional,\n",
    "                 dropout_rate):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                           num_layers=num_layers, bidirectional=bidirectional,\n",
    "                           dropout=dropout_rate)\n",
    "            \n",
    "    def forward(self, x, length):\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, length)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_x)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        return hidden, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hu3h2s29-nmb"
   },
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, bidirectional, out_dim,\n",
    "                 dropout_rate, pretrained_embed):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embed, freeze=True)\n",
    "        self.rnn = LSTM(embed_size, hidden_size, num_layers, bidirectional,\n",
    "                        dropout_rate)\n",
    "        self.proj = nn.Linear(2*hidden_size, out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        text, text_length = x # text: [seq_len, bs]\n",
    "        text = text.permute(1, 0) # text: [bs, seq_len]\n",
    "        embed_x = self.embed(text) # embed_x: [bs, seq_len, embed_dim]\n",
    "        embed_x = embed_x.permute(1, 0, 2) # embed_x: [seq_len, bs, embed_dim]\n",
    "        hidden, _ = self.rnn(embed_x, text_length) # hidden: [2*num_layers, bs, hidden_size]\n",
    "        hidden = torch.cat((hidden[-1,:,:], hidden[-2,:,:]), dim=1)\n",
    "        return self.proj(self.dropout(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "MeEwmTNZC8n2",
    "outputId": "a2a57823-284c-45af-a2c9-7dda14da4df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begin!\n",
      "Epoch: 1/5. Total loss: 204.548. Validation Set Acc: 79.900%.\n",
      "Epoch: 2/5. Total loss: 162.675. Validation Set Acc: 80.560%.\n",
      "Epoch: 3/5. Total loss: 138.629. Validation Set Acc: 80.780%.\n",
      "Epoch: 4/5. Total loss: 122.616. Validation Set Acc: 80.940%.\n",
      "Epoch: 5/5. Total loss: 106.104. Validation Set Acc: 80.820%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "out_dim = 2\n",
    "dropout_rate = 0.2\n",
    "pretrained_embed = text_field.vocab.vectors\n",
    "lr = 0.01\n",
    "num_epoch = 5\n",
    "out_model_file = 'textrnn.pt'\n",
    "\n",
    "textrnn = TextRNN(embed_size, hidden_size, num_layers, bidirectional, out_dim,\n",
    "                  dropout_rate, pretrained_embed).to(device)\n",
    "opt = torch.optim.Adam(textrnn.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Training begin!\")\n",
    "loss_list = train_fun(textrnn, train_iter, valid_iter, num_epoch, opt, criterion,\n",
    "                      eva, out_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cYiNoteVHzVU",
    "outputId": "c81b13ac-ef59-4cc2-b12b-03c52c899790"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81315"
      ]
     },
     "execution_count": 260,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrnn.load_state_dict(torch.load('textrnn.pt'))\n",
    "eva(textrnn, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPef9ql9IK8y"
   },
   "source": [
    "# 分类器4:TextRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLXJtOTJRSug"
   },
   "outputs": [],
   "source": [
    "class TextRCNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, bidirectional,\n",
    "                 out_dim, dropout_rate, pretrained_embed):\n",
    "        super(TextRCNN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embed, freeze=True)\n",
    "        self.rnn = nn.GRU(input_size=embed_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, bidirectional=bidirectional,\n",
    "                          dropout=dropout_rate)\n",
    "        self.proj1 = nn.Linear(2 * hidden_size + embed_size, 2 * hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.proj2 = nn.Linear(2 * hidden_size, out_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.xavier_uniform_(self.proj1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.proj2.weight.data)\n",
    "        nn.init.constant_(self.proj1.bias.data, 0)\n",
    "        nn.init.constant_(self.proj2.bias.data, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        text, _ = x # text: [seq_len, bs]\n",
    "        embed_x = self.embed(text) # emebd_x: [seq_len, bs, embed_dim]\n",
    "        output, _ = self.rnn(embed_x) # output: [seq_len, bs, 2*hidden_size]\n",
    "        embed_x, output = embed_x.permute(1, 0, 2), output.permute(1, 0, 2) # [bs, seq_len, 2*hidden_size]\n",
    "        y = torch.cat((output, embed_x), 2) # y: [bs, seq_len, 2*hidden_size+embed_dim]\n",
    "        z = self.tanh(self.proj1(y)) # z: [bs, seq_len, 2*hidden_size]\n",
    "        z = z.permute(0, 2, 1) # z: [bs, 2*hidden_size, seq_len]\n",
    "        z = torch.max_pool1d(z, z.shape[-1]).squeeze(-1) # z: [bs, 2*hidden_size]\n",
    "        return self.proj2(self.dropout(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "f5dsFzflWJ57",
    "outputId": "5ca7964f-3359-4981-a35c-120dc466c8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begin!\n",
      "Epoch: 1/10. Total loss: 210.249. Validation Set Acc: 80.920%.\n",
      "Epoch: 2/10. Total loss: 163.259. Validation Set Acc: 80.240%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "out_dim = 2\n",
    "dropout_rate = 0.2\n",
    "pretrained_embed = text_field.vocab.vectors\n",
    "lr = 0.01\n",
    "num_epoch = 10\n",
    "out_model_file = 'textrcnn.pt'\n",
    "\n",
    "textrcnn = TextRCNN(embed_size, hidden_size, num_layers, bidirectional, out_dim,\n",
    "                   dropout_rate, pretrained_embed).to(device)\n",
    "opt = torch.optim.Adam(textrcnn.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Training begin!\")\n",
    "loss_list = train_fun(textrcnn, train_iter, valid_iter, num_epoch, opt, criterion,\n",
    "                      eva, out_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OYOLAln2esGS",
    "outputId": "435629e7-ada9-4b0f-f3e4-c68ad94f880d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80425"
      ]
     },
     "execution_count": 254,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrcnn.load_state_dict(torch.load('textrcnn.pt'))\n",
    "eva(textrcnn, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7bFGs9hfO7V"
   },
   "source": [
    "# 分类器5:TextHAN (GRU+Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQAYsJj8fXbP"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextHAN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, num_layers, bidirectional, out_dim,\n",
    "                 dropout_rate, pretrained_embed):\n",
    "        super(TextHAN, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embed, freeze=True)\n",
    "        self.rnn = nn.GRU(input_size=embed_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, bidirectional=bidirectional,\n",
    "                          dropout=dropout_rate)\n",
    "        self.proj1 = nn.Linear(2 * hidden_size, 2 * hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.u = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n",
    "        self.proj2 = nn.Linear(2 * hidden_size, out_dim)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        nn.init.xavier_uniform_(self.proj1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.proj2.weight.data)\n",
    "        nn.init.constant_(self.proj1.bias.data, 0.1)\n",
    "        nn.init.constant_(self.proj2.bias.data, 0.1)\n",
    "        nn.init.uniform_(self.u, -0.1, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        text, _ = x # text: [seq_len, bs]\n",
    "        embed_x = self.embed(text) # embed_x: [seq_len, bs, embed_dim]\n",
    "        rnn_x, _ = self.rnn(embed_x) # rnn_x: [seq_len, bs, hidden_size * 2]\n",
    "        rnn_x = rnn_x.permute(1, 0, 2) # rnn_x: [bs, seq_len, hidden_size * 2]\n",
    "        ut = self.tanh(self.proj1(rnn_x)) # ut: [bs, seq_len, hidden_size * 2]\n",
    "        alpha = torch.softmax(torch.matmul(ut, self.u), dim=1) # alpha: [bs, seq_len, 1]\n",
    "        s = torch.sum(alpha * rnn_x, dim=1) # s: [bs, hidden*2]\n",
    "        return self.proj2(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "M6jVW7K31Ew3",
    "outputId": "e4ffbca6-9eb0-4766-e087-f1543a0b9b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begin!\n",
      "Epoch: 1/10. Total loss: 192.921. Validation Set Acc: 81.140%.\n",
      "Epoch: 2/10. Total loss: 145.733. Validation Set Acc: 80.260%.\n",
      "Early Stop!\n"
     ]
    }
   ],
   "source": [
    "embed_size = 300\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "out_dim = 2\n",
    "dropout_rate = 0.2\n",
    "pretrained_embed = text_field.vocab.vectors\n",
    "lr = 0.01\n",
    "num_epoch = 10\n",
    "out_model_file = 'texthan.pt'\n",
    "\n",
    "texthan = TextHAN(embed_size, hidden_size, num_layers, bidirectional, out_dim,\n",
    "                  dropout_rate, pretrained_embed).to(device)\n",
    "opt = torch.optim.Adam(texthan.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Training begin!\")\n",
    "loss_list = train_fun(texthan, train_iter, valid_iter, num_epoch, opt, criterion,\n",
    "                      eva, out_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GwX4eiW55bAb",
    "outputId": "4c799c80-fdb7-4534-d40c-d3129903f1d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texthan.load_state_dict(torch.load('texthan.pt'))\n",
    "eva(texthan, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kixDXzEtLWVS"
   },
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dou0Ke5HLYdL"
   },
   "source": [
    "在该项目中实现了5个文本分类模型，测试集上分类的准确率分别为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZCb9KJEIx4K"
   },
   "source": [
    "- TextFNN：70.395%\n",
    "- TextCNN: 75.830%\n",
    "- TextRNN: 81.315%\n",
    "- TextRCNN: 80.425%\n",
    "- TextHAN: 81.740%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "文本分类项目.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
